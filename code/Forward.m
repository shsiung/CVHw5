function [output, act_h, act_a] = Forward(W, b, X)
% [OUT, act_h, act_a] = Forward(W, b, X) performs forward propogation on the
% input data 'X' uisng the network defined by weights and biases 'W' and 'b'
% (as generated by InitializeNetwork(..)).
% This function should return the final softmax output layer activations in OUT,
% as well as the hidden layer pre activations in 'act_a', and the hidden layer post
% activations in 'act_h'.
layer_n = length(W);
act_h = cell(layer_n+1,1);
act_a = cell(layer_n+1,1);
act_h(1) = mat2cell(X,size(X,1));
act_a(1) = mat2cell(X,size(X,1));
curr_input = X;
sigmoid = @(x) 1/(1+exp(-x));
for i = 2 : layer_n+1
    curr_act = W{i-1}*curr_input+b{i-1};
    act_a(i) = num2cell(curr_act,1);
    curr_input = arrayfun(sigmoid, curr_act);
    act_h(i) = num2cell(curr_input,1);
end
softmax = @(x) exp(x);
act_h(end) = num2cell(arrayfun(softmax,curr_act),1);
act_h(end) = num2cell(act_h{end}/sum(act_h{end}),1);
output = act_h(end);
end
